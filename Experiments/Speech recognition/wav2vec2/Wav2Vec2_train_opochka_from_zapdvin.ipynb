{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa\n",
    "!pip install pandas\n",
    "!pip install transformers\n",
    "!pip install jiwer\n",
    "!pip install scikit-learn\n",
    "!pip install torch\n",
    "!pip install datasets\n",
    "!pip install dataclasses\n",
    "!pip install typing\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "import pandas as pd\n",
    "from jiwer import wer, cer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, ClassLabel, load_metric\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing source files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all files, define new sort function to sort as \\[1, 2, 3 ... 100\\], because built-in function sorts strings as \\[1, 100, 101 ...\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_files(directory, file_with_text, inf):\n",
    "    with open(file_with_text, encoding='utf-16') as f:\n",
    "        text = f.readlines()\n",
    "    files = os.listdir(directory)\n",
    "    files_full = []\n",
    "    for filename in files:\n",
    "        if '.DS_Store' not in filename:\n",
    "            f = os.path.join(directory, filename)\n",
    "            files_full.append(f)\n",
    "    files_full.sort(key=natural_keys)\n",
    "    j = 0\n",
    "    dict_for_inf = []\n",
    "    for filename in tqdm(files_full):\n",
    "        if not '=' in text[j] and not 'нрзб' in text[j] and not '[' in text[j] and not '<' in text[j]:\n",
    "            x = text[j].replace('\\n', '').lower()\n",
    "            x = x.replace('.', ' ')\n",
    "            x = x.replace(',', ' ')\n",
    "            x = x.replace(':', ' ')\n",
    "            x = x.replace('?', ' ')\n",
    "            x = x.replace('!', ' ')\n",
    "            x = x.replace('–', ' ')\n",
    "            x = x.replace('-', ' ')\n",
    "            x = x.replace('ё', 'е')\n",
    "            x = re.sub('(\\s){2,}', ' ', x)\n",
    "            x = re.sub('\\(.*\\)', '', x)\n",
    "            x = x.rstrip()\n",
    "            dict_for_inf.append({'respondent':inf, 'path': filename, 'sentence': x})\n",
    "        j += 1  \n",
    "    return dict_for_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 487/487 [00:00<00:00, 244479.48it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 234/234 [00:00<00:00, 233905.42it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 529/529 [00:00<00:00, 264424.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 434/434 [00:00<00:00, 216886.45it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 709/709 [00:00<00:00, 177210.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 277/277 [00:00<00:00, 276822.07it/s]\n"
     ]
    }
   ],
   "source": [
    "enm = prepare_files('/content/input_opochka/new_mono_enm20180618', \n",
    "                    '/content/input_opochka/20180618_enm1930_1to487.txt', 'ENM1930')\n",
    "ive = prepare_files('/content/input_opochka/new_mono_ive20190702', \n",
    "                    '/content/input_opochka/20190702_ive1949_1to234.txt', 'IVE1949')\n",
    "onv = prepare_files('/content/input_opochka/new_mono_onv20180622', \n",
    "                    '/content/input_opochka/20180622_onv1972_1to529.txt', 'ONV1972')\n",
    "saf = prepare_files('/content/input_opochka/new_mono_saf20190701', \n",
    "                    '/content/input_opochka/20190701_saf1973_1to434.txt', 'IVE1949')\n",
    "tai = prepare_files('/content/input_opochka/new_mono_tai20190706', \n",
    "                    '/content/input_opochka/20190706_tai1955_1to167.txt', 'TAI1955')\n",
    "tve = prepare_files('/content/input_opochka/new_mono_tve20190702', \n",
    "                    '/content/input_opochka/20190702_tve1955_1to709.txt', 'TVE1955')\n",
    "vav = prepare_files('/content/input_opochka/new_mono_vav20180619', \n",
    "                    '/content/input_opochka/20180619_vav1949_1to277.txt', 'VAV1949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2256"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = enm + ive + onv + saf + tai + tve + vav\n",
    "len(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2256/2256 [01:49<00:00, 20.66it/s]\n"
     ]
    }
   ],
   "source": [
    "def speech_file_to_array_fn(batch):\n",
    "    speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16000)\n",
    "    batch[\"speech\"] = speech_array\n",
    "    batch[\"sentence\"] = batch[\"sentence\"]\n",
    "    return batch\n",
    "\n",
    "test_dataset = []\n",
    "for l in tqdm(all_data):\n",
    "    test_dataset.append(speech_file_to_array_fn(l))\n",
    "data = [d['speech'] for d in test_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(test_dataset, columns=['respondent', 'path', 'sentence', 'speech'])\n",
    "ds = Dataset.from_pandas(df[['sentence', 'speech']])\n",
    "ds = ds.train_test_split(test_size=0.3, seed=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1579 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/677 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"sentence\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "vocabs = ds.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=ds.column_names[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(set(vocabs[\"train\"][\"vocab\"][0]) | set(vocabs[\"test\"][\"vocab\"][0]))\n",
    "\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG_ID = \"ru\"\n",
    "MODEL_ID = \"bond005/wav2vec2-large-ru-golos-with-lm\"\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID, padding=True)\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch, processor):\n",
    "    audio = batch[\"speech\"]\n",
    "    batch[\"input_values\"] = processor(audio, sampling_rate=16000).input_values[0]\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1579 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/677 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = ds.map(lambda examples: prepare_dataset(examples, processor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",).to('cpu')\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",).to('cpu')\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100).to('cpu')\n",
    "        batch[\"labels\"] = labels.to('cpu')\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Katya\\AppData\\Local\\Temp\\ipykernel_1420\\1788232804.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n"
     ]
    }
   ],
   "source": [
    "wer_metric = load_metric(\"wer\")\n",
    "cer_metric = load_metric(\"cer\", revision=\"master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    \n",
    "    pred_str = [pred_str[i] for i in range(len(pred_str)) if len(label_str[i]) > 0]\n",
    "    label_str = [label_str[i] for i in range(len(label_str)) if len(label_str[i]) > 0]\n",
    "    \n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer, 'cer': cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1644: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained('/content/wav2vec2-large-ru-golos-with-lm-dialect-full/checkpoint-10410/', local_files_only=True)\n",
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir='./wav2vec2-large-ru-golos-with-lm-shetnevo_and_opochka',\n",
    "  per_device_train_batch_size=8,\n",
    "  learning_rate=1e-4,\n",
    "  evaluation_strategy=\"epoch\",\n",
    "  num_train_epochs=15,\n",
    "  report_to=\"none\",\n",
    "  save_strategy=\"epoch\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2970' max='2970' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2970/2970 34:29:48, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.702313</td>\n",
       "      <td>0.531165</td>\n",
       "      <td>0.249459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.616474</td>\n",
       "      <td>0.521760</td>\n",
       "      <td>0.241440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.212700</td>\n",
       "      <td>2.752938</td>\n",
       "      <td>0.509166</td>\n",
       "      <td>0.235312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.212700</td>\n",
       "      <td>3.163397</td>\n",
       "      <td>0.503587</td>\n",
       "      <td>0.233300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.212700</td>\n",
       "      <td>3.243788</td>\n",
       "      <td>0.493384</td>\n",
       "      <td>0.226074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.878800</td>\n",
       "      <td>3.531349</td>\n",
       "      <td>0.493384</td>\n",
       "      <td>0.227873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.878800</td>\n",
       "      <td>3.699901</td>\n",
       "      <td>0.497848</td>\n",
       "      <td>0.226897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.705300</td>\n",
       "      <td>3.605258</td>\n",
       "      <td>0.490355</td>\n",
       "      <td>0.222934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.705300</td>\n",
       "      <td>3.500510</td>\n",
       "      <td>0.488124</td>\n",
       "      <td>0.224214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.705300</td>\n",
       "      <td>3.982125</td>\n",
       "      <td>0.487327</td>\n",
       "      <td>0.222720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.604400</td>\n",
       "      <td>3.932575</td>\n",
       "      <td>0.481110</td>\n",
       "      <td>0.220312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.604400</td>\n",
       "      <td>3.897301</td>\n",
       "      <td>0.485095</td>\n",
       "      <td>0.221257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.528400</td>\n",
       "      <td>3.951778</td>\n",
       "      <td>0.482225</td>\n",
       "      <td>0.219549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.528400</td>\n",
       "      <td>4.059438</td>\n",
       "      <td>0.482385</td>\n",
       "      <td>0.218879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.528400</td>\n",
       "      <td>4.077184</td>\n",
       "      <td>0.482544</td>\n",
       "      <td>0.218909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2970, training_loss=0.7405658606326941, metrics={'train_runtime': 124221.3891, 'train_samples_per_second': 0.191, 'train_steps_per_second': 0.024, 'total_flos': 6.411286297934812e+18, 'train_loss': 0.7405658606326941, 'epoch': 15.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"bond005/wav2vec2-large-ru-golos-with-lm\"\n",
    "model = Wav2Vec2ForCTC.from_pretrained('/content/wav2vec2-large-ru-golos-with-lm-shetnevo_and_opochka/checkpoint-2970/', local_files_only=True)\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/677 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_to_result(batch):\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(batch[\"input_values\"]).unsqueeze(0)\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
    "    batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
    "\n",
    "    return batch\n",
    "\n",
    "results = ds[\"test\"].map(map_to_result, remove_columns=ds[\"test\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean WER:  0.5366484431689765\n",
      "Mean CER:  0.3036211593533453\n"
     ]
    }
   ],
   "source": [
    "wers = []\n",
    "cers = []\n",
    "\n",
    "\n",
    "for item in results:\n",
    "    if item['text'] != '' and item['text'] != ' ':\n",
    "        w = wer(item['text'], item['pred_str'])\n",
    "        wers.append(w)\n",
    "        c = cer(item['text'], item['pred_str'])\n",
    "        cers.append(c)\n",
    "\n",
    "print('Mean WER: ', sum(wers)/len(wers))\n",
    "print('Mean CER: ', sum(cers)/len(cers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = results.to_pandas()\n",
    "path = \"/content/wav2vec_shetnevo_opochka.xlsx\"\n",
    "writer = pd.ExcelWriter(path, engine = 'xlsxwriter')\n",
    "\n",
    "test_results.to_excel(writer) \n",
    "\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a spellchecker for the received transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 677/677 [02:11<00:00,  5.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from pyaspeller import YandexSpeller\n",
    "speller = YandexSpeller()\n",
    "transcrtiptions_spelled = []\n",
    "for t in tqdm(results['pred_str']):\n",
    "    transcrtiptions_spelled.append(speller.spelled(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean WER:  0.49522013704937784\n",
      "Mean CER:  0.30350973125330377\n"
     ]
    }
   ],
   "source": [
    "wers = []\n",
    "cers = []\n",
    "\n",
    "for i, transcrtiption_spelled in enumerate(transcrtiptions_spelled):\n",
    "    if results['text'][i] != '' and results['text'][i] != ' ':\n",
    "        w = wer(results['text'][i], transcrtiption_spelled)\n",
    "        wers.append(w)\n",
    "        c = cer(results['text'][i], transcrtiption_spelled)\n",
    "        cers.append(c)\n",
    "        results['pred_str'][i] = transcrtiption_spelled\n",
    "\n",
    "print('Mean WER: ', sum(wers)/len(wers))\n",
    "print('Mean CER: ', sum(cers)/len(cers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = results.to_pandas()\n",
    "path = \"/content/wav2vec_shetnevo_opochka_with_spellcheck.xlsx\"\n",
    "writer = pd.ExcelWriter(path, engine = 'xlsxwriter')\n",
    "\n",
    "test_results.to_excel(writer) \n",
    "\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
