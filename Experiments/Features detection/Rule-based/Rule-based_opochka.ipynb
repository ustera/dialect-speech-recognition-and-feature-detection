{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a8b436d",
   "metadata": {},
   "source": [
    "# Import all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install wiktionaryparser\n",
    "!pip install ipapy\n",
    "!pip install pymorphy2\n",
    "!pip install natasha\n",
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28f2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from wiktionaryparser import WiktionaryParser\n",
    "import re\n",
    "from ipapy.ipastring import IPAString\n",
    "import pymorphy2\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    Doc\n",
    ")\n",
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c29ea6",
   "metadata": {},
   "source": [
    "# Processing source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba607d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "enm = pd.read_excel('/content/enm1930_ner.xlsx')\n",
    "enm['tokens'] = enm['tokens'].apply(lambda x: x.replace(\"'\", ''))\n",
    "enm['tokens'] = enm['tokens'].apply(lambda x: x.replace(\"[\", ''))\n",
    "enm['tokens'] = enm['tokens'].apply(lambda x: x.replace(\"]\", ''))\n",
    "enm['tokens'] = enm['tokens'].apply(lambda x: x.replace(\",\", ''))\n",
    "enm['tags'] = enm['tags'].apply(lambda x: x.replace(\"]\", ''))\n",
    "enm['tags'] = enm['tags'].apply(lambda x: x.replace(\"[\", ''))\n",
    "enm['tags'] = enm['tags'].apply(lambda x: x.replace(\"'\", ''))\n",
    "enm['tags'] = enm['tags'].apply(lambda x: x.replace(\",\", ''))\n",
    "enm['split_split_sent'] = enm['tokens'].apply(lambda x: x.split())\n",
    "enm['split_ner'] = enm['tags'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9ebd9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      None\n",
       "1      None\n",
       "2      None\n",
       "3      None\n",
       "4      None\n",
       "       ... \n",
       "463    None\n",
       "464    None\n",
       "465    None\n",
       "466    None\n",
       "467    None\n",
       "Length: 468, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_json(x, y):\n",
    "    global enm_json\n",
    "    enm_json.append({'sentence': x, 'tags': y})\n",
    "\n",
    "enm_json = []\n",
    "enm.apply(lambda x: to_json(x['split_split_sent'], x['split_ner']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdc2adf",
   "metadata": {},
   "source": [
    "# Define features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7a960ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yakanye(token, parser_ipa):\n",
    "    try:\n",
    "        word = parser_ipa.fetch(token, 'russian')\n",
    "        ipa = word[0]['pronunciations']['text'][0].replace('IPA: ', '')\n",
    "        regex = r\"(\\[.*\\])\"\n",
    "        matches = re.findall(regex, ipa)[0][1:-1]\n",
    "        s_ipa = IPAString(unicode_string=matches, ignore=True)\n",
    "        last_vowel = ''\n",
    "        if_palatalized_diacritic_before_vowel = False\n",
    "        s_ipa_list = []\n",
    "        for c in s_ipa:\n",
    "            s_ipa_list.append(c.name)\n",
    "        count_v = 0\n",
    "        for each in s_ipa_list:\n",
    "            if 'vowel' in each:\n",
    "                count_v += 1\n",
    "        if count_v >1:\n",
    "            for i in range(len(s_ipa_list)):\n",
    "                if s_ipa_list[i] != 'primary-stress suprasegmental':\n",
    "                    if 'vowel' in s_ipa_list[i]:\n",
    "                        last_vowel = s_ipa_list[i]\n",
    "                        try:\n",
    "                            if s_ipa_list[i-1] == 'palatalized diacritic':\n",
    "                                if_palatalized_diacritic_before_vowel = True\n",
    "                        except:\n",
    "                            pass\n",
    "                else:\n",
    "                    break\n",
    "            if last_vowel == 'near-close near-front unrounded vowel' and if_palatalized_diacritic_before_vowel == True:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def v(token):\n",
    "    if 'в' in token:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def sh(token):\n",
    "    if 'щ' in token:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def instrumental(p, doc, i):\n",
    "    head = doc.tokens[i].id\n",
    "    is_in_PP = False\n",
    "    for token in doc.tokens:\n",
    "        if token.head_id == head and token.pos == 'ADP':\n",
    "            is_in_PP = True\n",
    "    if p.tag.POS == 'NOUN' and p.tag.case == 'ablt' and p.tag.number == 'plur' and is_in_PP == False:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def deixis(p):\n",
    "    d = ['тот', 'этот', 'там', 'тут', 'здесь']\n",
    "    if p.normal_form in d:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def third(p):\n",
    "    if p.tag.POS == 'VERB' and p.tag.person == '3per' and (p.tag.tense == 'pres' or p.tag.tense == 'futr') and p.tag.mood == 'indc' and not p.word.endswith('ся'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def want(p):\n",
    "    if p.tag.POS == 'VERB' and p.tag.number == 'plur' and p.tag.tense == 'pres' and p.normal_form == 'хотеть':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def infinitives(p, token):\n",
    "    if p.tag.POS == 'INFN' and (token.endswith('ти') or token.endswith('есть')):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def postfixum(p, token):\n",
    "    vow = ['у', 'е', 'ы', 'а', 'о', 'э', 'я', 'и', 'ю']\n",
    "    if p.tag.POS == 'VERB' and (token.endswith('сь') or token.endswith('ся')) and token[:-2][-1] in vow:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def participles(p, token):\n",
    "    if (p.tag.POS == 'VERB' or p.tag.POS == 'GRND') and ('ши' in token or 'дчи' in token):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def participle_agreement_without_aux(doc):\n",
    "    res = []\n",
    "    for i in range(len(doc.tokens)):\n",
    "        ptc = []\n",
    "        if 'VerbForm' in doc.tokens[i].feats.keys() and doc.tokens[i].feats['VerbForm'] == 'Part' and doc.tokens[i].feats['Voice'] == 'Pass' and 'Gender' in doc.tokens[i].feats.keys():\n",
    "            ptc.append([i, doc.tokens[i]])\n",
    "        all_childs = []\n",
    "        for p in ptc:\n",
    "            for j in range(len(doc.tokens)):\n",
    "                if doc.tokens[j].head_id == p[1].id and doc.tokens[j].rel == 'nsubj:pass' and 'Gender' in doc.tokens[j].feats.keys():\n",
    "                    all_childs.append([p[0], p[1], j, doc.tokens[j]])\n",
    "        for each in all_childs:\n",
    "            if each[1].feats['Gender'] != each[3].feats['Gender']:\n",
    "                res.append([each[0], each[2]])\n",
    "    return res\n",
    "\n",
    "\n",
    "def participle_agreement_with_aux(doc):            \n",
    "    res = []\n",
    "    for i in range(len(doc.tokens)):\n",
    "        ptc = []\n",
    "        if 'VerbForm' in doc.tokens[i].feats.keys() and doc.tokens[i].feats['VerbForm'] == 'Part' and doc.tokens[i].feats['Voice'] == 'Pass' and 'Gender' in doc.tokens[i].feats.keys():\n",
    "            ptc.append([i, doc.tokens[i]])\n",
    "        ptc_aux = []\n",
    "        for p in ptc:\n",
    "            for j in range(len(doc.tokens)):\n",
    "                if doc.tokens[j].head_id == p[1].id and doc.tokens[j].rel == 'aux:pass' and 'Gender' in doc.tokens[j].feats.keys():\n",
    "                    ptc_aux.append([p[0], p[1],  j, doc.tokens[j]])\n",
    "        all_childs = []\n",
    "        for p in ptc_aux:\n",
    "            for k in range(len(doc.tokens)):\n",
    "                if doc.tokens[k].head_id == p[1].id and doc.tokens[k].rel == 'nsubj:pass' and 'Gender' in doc.tokens[k].feats.keys():\n",
    "                    all_childs.append([p[0], p[1], p[2], p[3], k, doc.tokens[k]])\n",
    "        for each in all_childs:\n",
    "            if each[1].feats['Gender'] != each[3].feats['Gender'] or each[1].feats['Gender'] != each[5].feats['Gender'] or each[3].feats['Gender'] != each[5].feats['Gender']:\n",
    "                res.append([each[0], each[2], each[4]])\n",
    "    return res\n",
    "\n",
    "\n",
    "def govorit_na(doc):\n",
    "    res = []\n",
    "    verbs = []\n",
    "    for i in range(len(doc.tokens)):\n",
    "        if doc.tokens[i].lemma == 'говорить' or doc.tokens[i].lemma == 'сказать':\n",
    "            verbs.append([i, doc.tokens[i]])\n",
    "    for verb in verbs:\n",
    "        for i in range(len(doc.tokens)):\n",
    "            if verb[1].id == doc.tokens[i].head_id:\n",
    "                for j in range(len(doc.tokens)):\n",
    "                    if doc.tokens[j].head_id == doc.tokens[i].id and doc.tokens[j].lemma == 'на':\n",
    "                        res.append([verb[0], j])\n",
    "    return res\n",
    "\n",
    "\n",
    "def chodit_v(doc):\n",
    "    res = []\n",
    "    verbs = []\n",
    "    verbs_lexeme = ['ходить', 'пойти', 'ездить', 'сходить', 'идти', 'приехать', \n",
    "                    'ехать', 'приезжать', 'прийти', 'поехать', 'приходить']\n",
    "    nouns_lexeme = ['гриб', 'ягода', 'орех', 'клюква', 'дрова', 'малина', 'черника', 'черница', \n",
    "                    'сморода', 'смородина', 'брусника', 'брусница', 'земляника', 'земляница']\n",
    "    for i in range(len(doc.tokens)):\n",
    "        if doc.tokens[i].lemma in verbs_lexeme:\n",
    "            verbs.append([i, doc.tokens[i]])\n",
    "    verb_noun = []\n",
    "    for verb in verbs:\n",
    "        for i in range(len(doc.tokens)):\n",
    "            if doc.tokens[i].head_id == verb[1].id and doc.tokens[i].lemma in nouns_lexeme:\n",
    "                verb_noun.append([verb[0], verb[1], i, doc.tokens[i]])\n",
    "    for item in verb_noun:\n",
    "        for k in range(len(doc.tokens)):\n",
    "            if item[3].id == doc.tokens[k].head_id and doc.tokens[k].lemma == 'в':\n",
    "                res.append([item[0], item[2], k])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7f7fb",
   "metadata": {},
   "source": [
    "# Define an algorithm that invokes checks for the presence of a particular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8f74c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_features(sent, parser_ipa, morph):\n",
    "    tokens_ready = {}\n",
    "    text = ' '.join(sent)\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    doc.parse_syntax(syntax_parser)\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "\n",
    "    for i in range(len(sent)):\n",
    "        res_other = {}\n",
    "        res = {}\n",
    "        v_token = v(sent[i])\n",
    "        if v_token:\n",
    "            res['фонема /в/'] = v_token\n",
    "        sh_token = sh(sent[i])\n",
    "        if sh_token:\n",
    "            res['длинный [ш]'] = sh_token\n",
    "        yakanye_token = yakanye(sent[i], parser_ipa)\n",
    "        if yakanye_token:\n",
    "            res['яканье'] = yakanye_token\n",
    "        res_other['фонетика'] = res\n",
    "        \n",
    "        p = morph.parse(sent[i])[0]\n",
    "        \n",
    "        res = {}\n",
    "        instrumental_token = instrumental(p, doc, i)\n",
    "        if instrumental_token:\n",
    "            res['творительный множественного'] = instrumental_token\n",
    "        deixis_token = deixis(p)\n",
    "        if deixis_token:\n",
    "            res['дейктические'] = deixis_token\n",
    "        third_token = third(p)\n",
    "        if third_token:\n",
    "            res['формы 3 лица презенса'] = third_token\n",
    "        want_token = want(p)\n",
    "        if want_token:\n",
    "            res['формы глагола хотеть'] = want_token\n",
    "        infinitives_token = infinitives(p, sent[i])\n",
    "        if infinitives_token:\n",
    "            res['формы инфинитивов'] = infinitives_token\n",
    "        postfixum_token = postfixum(p, sent[i])\n",
    "        if postfixum_token:\n",
    "            res['возвратные суффиксы'] = postfixum_token\n",
    "        participles_token = participles(p, sent[i])\n",
    "        if participles_token:\n",
    "            res['формы причастий'] = participles_token\n",
    "        res_other['морфология'] = res\n",
    "        \n",
    "        tokens_ready[i] = {sent[i]: res_other}\n",
    "    \n",
    "    text = ' '.join(sent)\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    doc.parse_syntax(syntax_parser)\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "    \n",
    "    part1 = participle_agreement_without_aux(doc)\n",
    "    part2 = participle_agreement_with_aux(doc)\n",
    "    gov = govorit_na(doc)\n",
    "    chod = chodit_v(doc)\n",
    "    res_synt = {}\n",
    "        \n",
    "    if part1 != []:\n",
    "        res_synt['согласование причастий без связки'] = part1\n",
    "    if part2 != []:\n",
    "        res_synt['согласование причастий со связкой'] = part2\n",
    "    if gov != []:\n",
    "        res_synt['говорить на'] = gov\n",
    "    if chod != []:\n",
    "        res_synt['ходить в'] = chod\n",
    "    \n",
    "    return res_synt, tokens_ready\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c295b",
   "metadata": {},
   "source": [
    "# Function response aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7be11e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 468/468 [52:28<00:00,  6.73s/it]\n"
     ]
    }
   ],
   "source": [
    "emb = NewsEmbedding()\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "parser_ipa = WiktionaryParser()\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "for item in tqdm(enm_json):\n",
    "    synt, other = check_features(item['sentence'], parser_ipa, morph)\n",
    "    res_other_p = []\n",
    "    res_other_m = []\n",
    "    for f in range(len(other)):\n",
    "        p_idx = 'O'\n",
    "        m_idx = 'O'\n",
    "        for key, value in other[f].items():\n",
    "            phon = value['фонетика']\n",
    "            morphol = value['морфология']\n",
    "            if phon != {}:\n",
    "                for key, value in phon.items():\n",
    "                    if value == True:\n",
    "                        p_idx = 'B-PHON'\n",
    "            if morphol != {}:\n",
    "                for key, value in morphol.items():\n",
    "                    if value == True:\n",
    "                        p_idx = 'B-MORPH'\n",
    "        res_other_p.append(p_idx)  \n",
    "        res_other_m.append(m_idx)\n",
    "    final_res_morph = []\n",
    "    for i in range(len(res_other_p)):\n",
    "        if res_other_p[i] == 'O' and res_other_m[i] == 'O':\n",
    "            final_res_morph.append('O')\n",
    "        elif res_other_p[i] != 'O' and res_other_m[i] == 'O':\n",
    "            final_res_morph.append(res_other_p[i])\n",
    "        elif res_other_p[i] == 'O' and res_other_m[i] != 'O':\n",
    "            final_res_morph.append(res_other_m[i])\n",
    "        else:\n",
    "            final_res_morph.append(res_other_m[i])\n",
    "    final_res_synt = ['O']*len(item['sentence'])\n",
    "    for key, value in synt.items():\n",
    "        if len(value) == 1:\n",
    "            res_value = sorted(value[0])\n",
    "            final_res_synt[res_value[0]] = 'B-SYNT'\n",
    "            for i in range(len(res_value[1:])):\n",
    "                final_res_synt[res_value[i]] = 'I-SYNT'\n",
    "        if len(value) > 1:\n",
    "            all_lists = list(set(sum(value, [])))\n",
    "            for seq in value:\n",
    "                seq = sorted(seq)\n",
    "                for i in range(len(seq)):\n",
    "                    if i == 0 and final_res_synt[seq[i]] != 'I-SYNT':\n",
    "                        final_res_synt[seq[i]] = 'B-SYNT'\n",
    "                    else:\n",
    "                        final_res_synt[seq[i]] = 'I-SYNT'\n",
    "\n",
    "    final = []\n",
    "    for i in range(len(final_res_morph)):\n",
    "        if final_res_morph[i] == 'O' and final_res_synt[i] == 'O':\n",
    "            final.append('O')\n",
    "        elif final_res_morph[i] != 'O' and final_res_synt[i] == 'O':\n",
    "            final.append(final_res_morph[i])\n",
    "        elif final_res_morph[i] == 'O' and final_res_synt[i] != 'O':\n",
    "            final.append(final_res_synt[i])\n",
    "        else:\n",
    "            final.append(final_res_synt[i])\n",
    "    item['predicted'] = final\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b4fac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enm_df = pd.DataFrame.from_records(enm_json)\n",
    "enm_df.to_excel('/content/enm1930_ner_3.xlsx') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697590a0",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd3c9bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LEX       0.00      0.00      0.00        42\n",
      "       MORPH       0.59      0.71      0.64       206\n",
      "        PHON       0.27      0.71      0.39       248\n",
      "        SYNT       0.60      0.12      0.21        24\n",
      "\n",
      "   micro avg       0.35      0.63      0.45       520\n",
      "   macro avg       0.36      0.39      0.31       520\n",
      "weighted avg       0.39      0.63      0.45       520\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "true_predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for x in enm_json:\n",
    "    true_predictions.append(x['predicted'])\n",
    "    true_labels.append(x['tags'])\n",
    "    \n",
    "print(classification_report(true_labels, true_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ded7c8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DIAL       0.38      0.67      0.49       520\n",
      "\n",
      "   micro avg       0.38      0.67      0.49       520\n",
      "   macro avg       0.38      0.67      0.49       520\n",
      "weighted avg       0.38      0.67      0.49       520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_dial_preds = []\n",
    "for y in true_predictions:\n",
    "    r = []\n",
    "    for x in y:\n",
    "        if x != 'O' and x[0] == 'B':\n",
    "            r.append('B-DIAL')\n",
    "        elif x != 'O' and x[0] == 'I':\n",
    "            r.append('I-DIAL')\n",
    "        else:\n",
    "            r.append('O')\n",
    "    true_dial_preds.append(r)\n",
    "\n",
    "true_dial_labels = []\n",
    "for y in true_labels:\n",
    "    r = []\n",
    "    for x in y:\n",
    "        if x != 'O' and x[0] == 'B':\n",
    "            r.append('B-DIAL')\n",
    "        elif x != 'O' and x[0] == 'I':\n",
    "            r.append('I-DIAL')\n",
    "        else:\n",
    "            r.append('O')\n",
    "    true_dial_labels.append(r)\n",
    "print(classification_report(true_dial_labels, true_dial_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638646ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
