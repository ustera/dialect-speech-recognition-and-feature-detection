{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d016c25a",
   "metadata": {},
   "source": [
    "# Import all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc24079",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install wiktionaryparser\n",
    "!pip install ipapy\n",
    "!pip install pymorphy2\n",
    "!pip install natasha\n",
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa5a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from wiktionaryparser import WiktionaryParser\n",
    "import re\n",
    "from ipapy.ipastring import IPAString\n",
    "import pymorphy2\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    Doc\n",
    ")\n",
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df472de8",
   "metadata": {},
   "source": [
    "# Processing source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02890d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mga = pd.read_excel('/content/mga_ner.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caea9263",
   "metadata": {},
   "outputs": [],
   "source": [
    "mga['split_sent'] = mga['split_sent'].apply(lambda x: x.replace(\"'\", ''))\n",
    "mga['split_sent'] = mga['split_sent'].apply(lambda x: x.replace(\"[\", ''))\n",
    "mga['split_sent'] = mga['split_sent'].apply(lambda x: x.replace(\"]\", ''))\n",
    "mga['split_sent'] = mga['split_sent'].apply(lambda x: x.replace(\",\", ''))\n",
    "mga['ner'] = mga['ner'].apply(lambda x: x.replace(\"]\", ''))\n",
    "mga['ner'] = mga['ner'].apply(lambda x: x.replace(\"[\", ''))\n",
    "mga['ner'] = mga['ner'].apply(lambda x: x.replace(\"'\", ''))\n",
    "mga['ner'] = mga['ner'].apply(lambda x: x.replace(\",\", ''))\n",
    "mga['split_split_sent'] = mga['split_sent'].apply(lambda x: x.split())\n",
    "mga['split_ner'] = mga['ner'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f3686a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "2       None\n",
       "3       None\n",
       "4       None\n",
       "        ... \n",
       "1439    None\n",
       "1440    None\n",
       "1441    None\n",
       "1442    None\n",
       "1443    None\n",
       "Length: 1444, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_json(x, y):\n",
    "    global mga_json\n",
    "    mga_json.append({'sentence': x, 'tags': y})\n",
    "\n",
    "mga_json = []\n",
    "mga.apply(lambda x: to_json(x['split_split_sent'], x['split_ner']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd8e31c",
   "metadata": {},
   "source": [
    "# Define features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1c40a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yakanye(token, parser_ipa):\n",
    "    try:\n",
    "        word = parser_ipa.fetch(token, 'russian')\n",
    "        ipa = word[0]['pronunciations']['text'][0].replace('IPA: ', '')\n",
    "        regex = r\"(\\[.*\\])\"\n",
    "        matches = re.findall(regex, ipa)[0][1:-1]\n",
    "        s_ipa = IPAString(unicode_string=matches, ignore=True)\n",
    "        last_vowel = ''\n",
    "        if_palatalized_diacritic_before_vowel = False\n",
    "        s_ipa_list = []\n",
    "        for c in s_ipa:\n",
    "            s_ipa_list.append(c.name)\n",
    "        count_v = 0\n",
    "        for each in s_ipa_list:\n",
    "            if 'vowel' in each:\n",
    "                count_v += 1\n",
    "        if count_v >1:\n",
    "            for i in range(len(s_ipa_list)):\n",
    "                if s_ipa_list[i] != 'primary-stress suprasegmental':\n",
    "                    if 'vowel' in s_ipa_list[i]:\n",
    "                        last_vowel = s_ipa_list[i]\n",
    "                        try:\n",
    "                            if s_ipa_list[i-1] == 'palatalized diacritic':\n",
    "                                if_palatalized_diacritic_before_vowel = True\n",
    "                        except:\n",
    "                            pass\n",
    "                else:\n",
    "                    break\n",
    "            if last_vowel == 'near-close near-front unrounded vowel' and if_palatalized_diacritic_before_vowel == True:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def v(token):\n",
    "    if 'в' in token:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def sh(token):\n",
    "    if 'щ' in token:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def instrumental(p):\n",
    "    if p.tag.POS == 'NOUN' and p.tag.case == 'ablt' and p.tag.number == 'plur':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def deixis(p):\n",
    "    d = ['тот', 'этот', 'там', 'тут', 'здесь']\n",
    "    if p.normal_form in d:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def third(p):\n",
    "    if p.tag.POS == 'VERB' and p.tag.person == '3per' and p.tag.tense == 'pres':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def want(p):\n",
    "    if p.tag.POS == 'VERB' and p.tag.number == 'plur' and p.tag.tense == 'pres' and p.normal_form == 'хотеть':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def infinitives(p, token):\n",
    "    if p.tag.POS == 'INFN' and (token.endswith('ти') or token.endswith('есть')):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def postfixum(p, token):\n",
    "    vow = ['у', 'е', 'ы', 'а', 'о', 'э', 'я', 'и', 'ю']\n",
    "    if p.tag.POS == 'VERB' and (token.endswith('cь') or token.endswith('cя')) and token[:-2][-1] in vow:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def participles(p, token):\n",
    "    if (p.tag.POS == 'VERB' or p.tag.POS == 'GRND') and ('вши' in token or 'лши' in token or 'дчи' in token):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def participle_agreement_without_aux(doc):\n",
    "    res = []\n",
    "    for i in range(len(doc.tokens)):\n",
    "        ptc = []\n",
    "        if 'VerbForm' in doc.tokens[i].feats.keys() and doc.tokens[i].feats['VerbForm'] == 'Part' and doc.tokens[i].feats['Voice'] == 'Pass':\n",
    "            ptc.append([i, doc.tokens[i]])\n",
    "        all_childs = []\n",
    "        for p in ptc:\n",
    "            for j in range(len(doc.tokens)):\n",
    "                if doc.tokens[j].head_id == p[1].id and doc.tokens[j].rel == 'nsubj:pass' and 'Gender' in doc.tokens[j].feats.keys():\n",
    "                    all_childs.append([p[0], p[1], j, doc.tokens[j]])\n",
    "        for each in all_childs:\n",
    "            if each[1].feats['Gender'] != each[3].feats['Gender']:\n",
    "                res.append([each[0], each[2]])\n",
    "    return res\n",
    "\n",
    "\n",
    "def participle_agreement_with_aux(doc):            \n",
    "    res = []\n",
    "    for i in range(len(doc.tokens)):\n",
    "        ptc = []\n",
    "        if 'VerbForm' in doc.tokens[i].feats.keys() and doc.tokens[i].feats['VerbForm'] == 'Part' and doc.tokens[i].feats['Voice'] == 'Pass':\n",
    "            ptc.append([i, doc.tokens[i]])\n",
    "        ptc_aux = []\n",
    "        for p in ptc:\n",
    "            for j in range(len(doc.tokens)):\n",
    "                if doc.tokens[j].head_id == p[1].id and doc.tokens[j].rel == 'aux:pass':\n",
    "                    ptc_aux.append([p[0], p[1],  j, doc.tokens[j]])\n",
    "        all_childs = []\n",
    "        for p in ptc_aux:\n",
    "            for k in range(len(doc.tokens)):\n",
    "                if doc.tokens[k].head_id == p[1].id and doc.tokens[k].rel == 'nsubj:pass' and 'Gender' in doc.tokens[k].feats.keys():\n",
    "                    all_childs.append([p[0], p[1], p[2], p[3], k, doc.tokens[k]])\n",
    "        for each in all_childs:\n",
    "            if each[1].feats['Gender'] != each[3].feats['Gender'] or each[1].feats['Gender'] != each[5].feats['Gender'] or each[3].feats['Gender'] != each[5].feats['Gender']:\n",
    "                res.append([each[0], each[2], each[4]])\n",
    "    return res\n",
    "\n",
    "\n",
    "def nominative_obj(doc):\n",
    "    res = []\n",
    "    for i in range(len(doc.tokens)):\n",
    "        need = []\n",
    "        if doc.tokens[i].lemma == 'надо':\n",
    "            need.append([i, doc.tokens[i]])\n",
    "        need_verb = []\n",
    "        for p in need:\n",
    "            for j in range(len(doc.tokens)):\n",
    "                if doc.tokens[j].head_id == p[1].id and doc.tokens[j].rel == 'csubj':\n",
    "                    need_verb.append([i, p[1], j, doc.tokens[j]])\n",
    "        all_childs = []\n",
    "        for p in need_verb:\n",
    "            for k in range(len(doc.tokens)):\n",
    "                if doc.tokens[k].head_id == p[1].id and doc.tokens[k].rel == 'nsubj':\n",
    "                    all_childs.append([p[0], p[2], k])\n",
    "\n",
    "        if all_childs != []:\n",
    "            res.append(all_childs)\n",
    "    return res\n",
    "\n",
    "\n",
    "def govorit_na(doc):\n",
    "    res = []\n",
    "    verbs = []\n",
    "    for i in range(len(doc.tokens)):\n",
    "        if doc.tokens[i].lemma == 'говорить':\n",
    "            verbs.append([i, doc.tokens[i]])\n",
    "    for verb in verbs:\n",
    "        for i in range(len(doc.tokens)):\n",
    "            if verb[1].id == doc.tokens[i].head_id:\n",
    "                for j in range(len(doc.tokens)):\n",
    "                    if doc.tokens[j].head_id == doc.tokens[i].id and doc.tokens[j].lemma == 'на':\n",
    "                        res.append([verb[0], j])\n",
    "    return res\n",
    "\n",
    "\n",
    "def chodit_v(doc):\n",
    "    res = []\n",
    "    verbs = []\n",
    "    for i in range(len(doc.tokens)):\n",
    "        if doc.tokens[i].lemma == 'ходить':\n",
    "            verbs.append([i, doc.tokens[i]])\n",
    "    for verb in verbs:\n",
    "        for i in range(len(doc.tokens)):\n",
    "            if verb[1].id == doc.tokens[i].head_id:\n",
    "                for j in range(len(doc.tokens)):\n",
    "                    if doc.tokens[j].head_id == doc.tokens[i].id and doc.tokens[j].lemma == 'в':\n",
    "                        res.append([verb[0], j])\n",
    "    return res\n",
    "\n",
    "\n",
    "def plusquamperfect(doc):\n",
    "    res = []\n",
    "    verbs = []\n",
    "    for i in range(len(doc.tokens)):\n",
    "        if doc.tokens[i].pos == 'VERB':\n",
    "            verbs.append([i, doc.tokens[i]])\n",
    "    all_children = []\n",
    "    for verb in verbs:\n",
    "        for i in range(len(doc.tokens)):\n",
    "            if verb[1].id == doc.tokens[i].head_id and doc.tokens[i].rel == 'cop':\n",
    "                res.append([i, verb[0]])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8143b25",
   "metadata": {},
   "source": [
    "# Define an algorithm that invokes checks for the presence of a particular features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6cd6bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_features(sent, parser_ipa, morph):\n",
    "    tokens_ready = {}\n",
    "    for i in range(len(sent)):\n",
    "        res_other = {}\n",
    "        res = {}\n",
    "        v_token = v(sent[i])\n",
    "        if v_token:\n",
    "            res['фонема /в/'] = v_token\n",
    "        sh_token = sh(sent[i])\n",
    "        if sh_token:\n",
    "            res['длинный [ш]'] = sh_token\n",
    "        yakanye_token = yakanye(sent[i], parser_ipa)\n",
    "        if yakanye_token:\n",
    "            res['яканье'] = yakanye_token\n",
    "        res_other['фонетика'] = res\n",
    "        \n",
    "        p = morph.parse(sent[i])[0]\n",
    "        \n",
    "        res = {}\n",
    "        instrumental_token = instrumental(p)\n",
    "        if instrumental_token:\n",
    "            res['творительный множественного'] = instrumental_token\n",
    "        deixis_token = deixis(p)\n",
    "        if deixis_token:\n",
    "            res['дейктические'] = deixis_token\n",
    "        third_token = third(p)\n",
    "        if third_token:\n",
    "            res['формы 3 лица презенса'] = third_token\n",
    "        want_token = want(p)\n",
    "        if want_token:\n",
    "            res['формы глагола хотеть'] = want_token\n",
    "        infinitives_token = infinitives(p, sent[i])\n",
    "        if infinitives_token:\n",
    "            res['формы инфинитивов'] = infinitives_token\n",
    "        postfixum_token = postfixum(p, sent[i])\n",
    "        if postfixum_token:\n",
    "            res['возвратные суффиксы'] = postfixum_token\n",
    "        participles_token = participles(p, sent[i])\n",
    "        if participles_token:\n",
    "            res['формы причастий'] = participles_token\n",
    "        res_other['морфология'] = res\n",
    "        \n",
    "        tokens_ready[i] = {sent[i]: res_other}\n",
    "    \n",
    "    text = ' '.join(sent)[1:]\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    doc.parse_syntax(syntax_parser)\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "    \n",
    "    part1 = participle_agreement_without_aux(doc)\n",
    "    part2 = participle_agreement_with_aux(doc)          \n",
    "    nom = nominative_obj(doc)\n",
    "    gov = govorit_na(doc)\n",
    "    chod = chodit_v(doc)\n",
    "    pqpf = plusquamperfect(doc)\n",
    "    \n",
    "    res_synt = {}\n",
    "    \n",
    "    if part1 != []:\n",
    "        res_synt['согласование причастий без связки'] = part1\n",
    "    if part2 != []:\n",
    "        res_synt['согласование причастий со связкой'] = part2\n",
    "    if nom != []:\n",
    "        res_synt['номинативный объект'] = nom\n",
    "    if gov != []:\n",
    "        res_synt['говорить на'] = gov\n",
    "    if chod != []:\n",
    "        res_synt['ходить в'] = chod\n",
    "    if pqpf != []:\n",
    "        res_synt['плюсквамперфект'] = pqpf\n",
    "    \n",
    "    return res_synt, tokens_ready\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a3e0d",
   "metadata": {},
   "source": [
    "# Function response aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3e9728b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1444/1444 [45:52<00:00,  1.91s/it]\n"
     ]
    }
   ],
   "source": [
    "emb = NewsEmbedding()\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "parser_ipa = WiktionaryParser()\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "for item in tqdm(mga_json):\n",
    "    synt, other = check_features(item['sentence'], parser_ipa, morph)\n",
    "    res_other_p = []\n",
    "    res_other_m = []\n",
    "    for f in range(len(other)):\n",
    "        p_idx = 'O'\n",
    "        m_idx = 'O'\n",
    "        for key, value in other[f].items():\n",
    "            phon = value['фонетика']\n",
    "            morphol = value['морфология']\n",
    "            if phon != {}:\n",
    "                for key, value in phon.items():\n",
    "                    if value == True:\n",
    "                        p_idx = 'B-PHON'\n",
    "            if morphol != {}:\n",
    "                for key, value in morphol.items():\n",
    "                    if value == True:\n",
    "                        p_idx = 'B-MORPH'\n",
    "        res_other_p.append(p_idx)  \n",
    "        res_other_m.append(m_idx)\n",
    "    final_res_morph = []\n",
    "    for i in range(len(res_other_p)):\n",
    "        if res_other_p[i] == 'O' and res_other_m[i] == 'O':\n",
    "            final_res_morph.append('O')\n",
    "        elif res_other_p[i] != 'O' and res_other_m[i] == 'O':\n",
    "            final_res_morph.append(res_other_p[i])\n",
    "        elif res_other_p[i] == 'O' and res_other_m[i] != 'O':\n",
    "            final_res_morph.append(res_other_m[i])\n",
    "        else:\n",
    "            final_res_morph.append(res_other_m[i])\n",
    "    final_res_synt = ['O']*len(item['sentence'])\n",
    "    for key, value in synt.items():\n",
    "        if len(value) == 1:\n",
    "            res_value = value[0].sort()\n",
    "            final_res_synt[value[0][0]] = 'B-SYNT'\n",
    "            for i in range(value[0][0]+1, value[0][1]+1):\n",
    "                final_res_synt[i] = 'I-SYNT'\n",
    "        if len(value) > 1:\n",
    "            all_lists = list(set(sum(value, [])))\n",
    "            for seq in value:\n",
    "                seq = sorted(seq)\n",
    "                for i in range(len(seq)):\n",
    "                    if i == 0 and final_res_synt[seq[i]] == 'O':\n",
    "                        final_res_synt[seq[i]] = 'B-SYNT'\n",
    "                    else:\n",
    "                        final_res_synt[seq[i]] = 'I-SYNT'\n",
    "                \n",
    "    final = []\n",
    "    for i in range(len(final_res_morph)):\n",
    "        if final_res_morph[i] == 'O' and final_res_synt[i] == 'O':\n",
    "            final.append('O')\n",
    "        elif final_res_morph[i] != 'O' and final_res_synt[i] == 'O':\n",
    "            final.append(final_res_morph[i])\n",
    "        elif final_res_morph[i] == 'O' and final_res_synt[i] != 'O':\n",
    "            final.append(final_res_synt[i])\n",
    "        else:\n",
    "            final.append(final_res_synt[i])\n",
    "    item['predicted'] = final\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe59015",
   "metadata": {},
   "source": [
    "# Test and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8b41c672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LEX       0.00      0.00      0.00         7\n",
      "       MORPH       0.41      0.52      0.46       182\n",
      "        PHON       0.07      0.51      0.13       156\n",
      "        SYNT       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.13      0.49      0.21       358\n",
      "   macro avg       0.12      0.26      0.15       358\n",
      "weighted avg       0.24      0.49      0.29       358\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Katya\\anaconda3\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "true_predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for x in mga_json:\n",
    "    true_predictions.append(x['predicted'])\n",
    "    true_labels.append(x['tags'])\n",
    "    \n",
    "print(classification_report(true_labels, true_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "67fadd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DIAL       0.15      0.55      0.23       357\n",
      "\n",
      "   micro avg       0.15      0.55      0.23       357\n",
      "   macro avg       0.15      0.55      0.23       357\n",
      "weighted avg       0.15      0.55      0.23       357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_dial_preds = []\n",
    "for y in true_predictions:\n",
    "    r = []\n",
    "    for x in y:\n",
    "        if x != 'O' and x[0] == 'B':\n",
    "            r.append('B-DIAL')\n",
    "        elif x != 'O' and x[0] == 'I':\n",
    "            r.append('I-DIAL')\n",
    "        else:\n",
    "            r.append('O')\n",
    "    true_dial_preds.append(r)\n",
    "\n",
    "true_dial_labels = []\n",
    "for y in true_labels:\n",
    "    r = []\n",
    "    for x in y:\n",
    "        if x != 'O' and x[0] == 'B':\n",
    "            r.append('B-DIAL')\n",
    "        elif x != 'O' and x[0] == 'I':\n",
    "            r.append('I-DIAL')\n",
    "        else:\n",
    "            r.append('O')\n",
    "    true_dial_labels.append(r)\n",
    "print(classification_report(true_dial_labels, true_dial_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
